{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Group by is being handled as part of this code below\n",
    "\n",
    "\n",
    "### About moz parser\n",
    "SQL is a familiar language used to access databases. Although, each database vendor has its quirky implementation, the average developer does not know enough SQL to be concerned with those quirks. This familiar core SQL (lowest common denominator, if you will) is useful enough to explore data in primitive ways. It is hoped that, once programmers have reviewed a datastore with basic SQL queries, and they see the value of that data, they will be motivated to use the datastore's native query format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moz_sql_parser import parse\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate_select_dict will give a dictionary where all the base columns + udf and alias will be seggregated, it can accomodate 4 level of UDF for now, it is basically the driver of the select ang group part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_select_dict(select_list):\n",
    "    column_list = []\n",
    "    for column_dict in select_list:\n",
    "        if column_dict==\"*\":\n",
    "            pass\n",
    "        elif type(column_dict['value'])== str:\n",
    "            if '.' in column_dict['value']:\n",
    "                column_value = column_dict['value'].split('.')[1]\n",
    "                column_table = column_dict['value'].split('.')[0]\n",
    "                try:\n",
    "                    alias=column_dict['name']\n",
    "                except:\n",
    "                    alias=\"\"\n",
    "                collsttmp={\"base_col\":column_value,\"udf\":\"\",\"Alias\":alias}\n",
    "                column_list.append(collsttmp)\n",
    "            else:\n",
    "                column_value = column_dict['value']\n",
    "                column_table=\"\"\n",
    "                try:\n",
    "                    alias=column_dict['name']\n",
    "                except:\n",
    "                    alias=\"\"\n",
    "                collsttmp={\"base_col\":column_value,\"udf\":\"\",\"Alias\":alias}\n",
    "                column_list.append(collsttmp)\n",
    "            \n",
    "            \n",
    "        elif type(column_dict['value'])== dict:\n",
    "            if '.' in column_dict['value']:\n",
    "                column_value = column_dict['value'].split('.')[1]\n",
    "                column_table = column_dict['value'].split('.')[0]\n",
    "                try:\n",
    "                    alias=column_dict['name']\n",
    "                except:\n",
    "                    alias=\"\"\n",
    "                colsttmp={\"base_col\":column_value, \"Table\":column_table,\"Alias\":alias}\n",
    "                column_list.append(colsttmp)\n",
    "            else:\n",
    "                column_value=column_dict['value']\n",
    "                final_col=[]\n",
    "                for k,v in column_dict['value'].items():\n",
    "                    if k==\"case\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        udf=k\n",
    "                        cols=v\n",
    "                        if type(cols)==str:\n",
    "                            if '.' in cols:\n",
    "                                col_name =cols.split('.')\n",
    "                                final_col.append(col_name[1])\n",
    "                            else:\n",
    "                                final_col.append(cols)\n",
    "                        elif type(cols)==dict:\n",
    "                            for k,v in cols.items(): ###########needs to be coded\n",
    "                                udf=udf+\",\"+k\n",
    "                                cols=v\n",
    "                                for i in cols:\n",
    "                                    final_col.append(i)\n",
    "                                \n",
    "                        else:\n",
    "                            for i in cols:\n",
    "                                if type(i)==str:\n",
    "                                    if '.' in i:\n",
    "                                        column_name= i.split('.')\n",
    "                                        col_name= column_name[1]\n",
    "                                        final_col.append(col_name)\n",
    "                                    else:\n",
    "                                        final_col.append(i)\n",
    "                        \n",
    "                                elif type(i)==int:\n",
    "                                    final_col.append(i)\n",
    "                    \n",
    "                                elif type(i)==dict: ## here adjustments needs to be done\n",
    "                                    new_dict=i\n",
    "                                    for k,v in new_dict.items():\n",
    "                                        extra_udf=k\n",
    "                                        udf=udf+\",\"+extra_udf\n",
    "                                        cols=v\n",
    "                                        if type(cols)==list:  ## for list\n",
    "                                            for i in cols:\n",
    "                                                if '.' in i:\n",
    "                                                    splitter= i.split('.')\n",
    "                                                    part1=splitter[0] \n",
    "                                                    part2=splitter[1]\n",
    "                                                    final_col.append(part2)\n",
    "                                                else:\n",
    "                                                    final_col.append(i)\n",
    "                                        elif type(cols)==str:  ## for str\n",
    "                                            if '.' in cols:\n",
    "                                                splitter= cols.split('.')\n",
    "                                                part1=splitter[0] \n",
    "                                                part2=splitter[1]\n",
    "                                                final_col.append(part2)\n",
    "                                            else:\n",
    "                                                final_col.append(cols)\n",
    "                                        elif type(cols)==dict:  ## for dict\n",
    "                                            for k,v in cols.items():\n",
    "                                                third_udf=k\n",
    "                                                udf =udf+\",\"+third_udf\n",
    "                                                cols=v\n",
    "                                                for i in cols:\n",
    "                                                    if '.' in i:\n",
    "                                                        splitter= i.split('.')\n",
    "                                                        part1=splitter[0] \n",
    "                                                        part2=splitter[1]\n",
    "                                                        final_col.append(part2)\n",
    "                                                    else:\n",
    "                                                        final_col.append(i)\n",
    "                                        else:\n",
    "                                            pass\n",
    "                                            \n",
    "                                else:\n",
    "                                    pass\n",
    "                    try:\n",
    "                        alias=column_dict['name']\n",
    "                    except:\n",
    "                        alias=\"\"\n",
    "                    colltmp={\"base_col\":final_col, \"udf\":udf,\"Alias\":alias}\n",
    "                    column_list.append(colltmp)\n",
    "    return column_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the required UDF's are incorporated here, this list will be continued further..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce_udf(columns,final_df,alias,final_columns,len_udf):\n",
    "    coalesce_filler=str(columns[-1])\n",
    "    if len_udf==1:\n",
    "        query =final_df+\"[`\"+alias+\"`]\"+\"=\"+final_df+\".\"+final_columns[0]+\".fillna(value=\"+coalesce_filler+',inplace=True)'\n",
    "    else:\n",
    "        query =final_df+\"[`\"+alias+\"`]\"+\"=\"+final_df+\".\"+alias+\".fillna(value=\"+coalesce_filler+',inplace=True)'\n",
    "    return query\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplication_udf(final_columns,alias,final_df):\n",
    "    list_of_col = [\"row.\"+a for a in final_columns]\n",
    "    cols='*'.join(list_of_col)\n",
    "    query = final_df+\"[`\"+alias+\"`]\"+\"=\"+final_df+'.apply(lambda row: '+cols+', axis = 1)'\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic sum functionality of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_initial_udf(columns,final_df,alias):\n",
    "    columns=columns[0]\n",
    "    query= final_df+\"[`\"+alias+\"`]\"+\"=\"+final_df+\"[`\"+columns+\"`]\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year and month udf's are being handled here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_month_udf(final_df,alias,udf,final_columns):\n",
    "    query =final_df+\"[`\"+alias+\"`]\"+\"=\"+final_df+\"[`\"+final_columns[0]+\"`]\"+\".dt.\"+udf+\")\"\n",
    "    return query\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Literal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def literals_adjust(final_df,columns,alias):\n",
    "    \"\"\"add new column to pandas dataframe with default value\"\"\"\n",
    "    columns=columns[0]\n",
    "    query =final_df+\"[`\"+alias+\"`]\"+\"=\"+\"`\"+columns+\"`\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct or unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_unique(final_df,alias,udf,final_columns):\n",
    "    if alias==\"\":\n",
    "        final_fcol=final_columns[0]\n",
    "        query =final_df+\"[`\"+final_fcol+\"`]\"+\"=\"+final_df+\"[`\"+final_fcol+\"`].unique()\"\n",
    "    else:\n",
    "        final_fcol=final_columns[0]\n",
    "        query =final_df+\"[`\"+alias+\"`]\"+\"=\"+final_df+\"[`\"+final_fcol+\"`].unique()\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping columns , this will return all the columns which needs to be grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_columns(group_section):\n",
    "    list1=[]\n",
    "    if 'groupby' in query_dict.keys():\n",
    "        if type(group_section)==list:\n",
    "            for i in group_section:\n",
    "                values =i['value']\n",
    "                list1.append(values)\n",
    "        elif type(group_section)==dict:\n",
    "            for k,v in group_section.items():\n",
    "                list1.append(v)\n",
    "    else:\n",
    "        pass\n",
    "    return list1\n",
    "#grouped_columns(group_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column on which group by needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not there\n",
    "def group_to_be_applied_on(sql_dict):\n",
    "    list_col=[]\n",
    "    for i in sql_dict:\n",
    "        all_udf = i['udf']\n",
    "        udf_splitter = all_udf.split(',')\n",
    "        list_of_agg=['sum','min','max','avg','mean','count']\n",
    "        for agg in list_of_agg:\n",
    "            if agg in i['udf']:\n",
    "                agg_cols = i['Alias']\n",
    "                list_col.append(agg_cols)\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "    return list_col\n",
    "#group_to_be_applied_on(sql_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function will do the group by and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_func(final_df,query_dict,grp_cols,alias,udf,final_columns):\n",
    "    if 'groupby' in query_dict.keys():\n",
    "        final_fcol=final_columns[0]\n",
    "        grp_by = final_df+\"[`\"+alias+\"`]\"+\"=\"+final_df+\".groupby(\"+str(grp_cols)+\")\"+\"[`\"+alias+\"`]\"+\".agg(\"+udf+\")\"\n",
    "    return grp_by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below function will return the pandas converted queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_constructor(final_df,grp_cols,sql_dict,group_agg):\n",
    "    query_list=[]\n",
    "    for list_elements in sql_dict:\n",
    "        #print(list_elements)\n",
    "        columns = list_elements['base_col']\n",
    "        if columns!='*':\n",
    "            if columns!=[]:\n",
    "                final_columns =[s for s in columns if type(s)==str]\n",
    "                alias=list_elements['Alias']\n",
    "                udf=list_elements['udf']\n",
    "                if list_elements['udf']!='':\n",
    "                    udf_splitter = list_elements['udf'].split(',')\n",
    "                    len_udf =len(udf_splitter)\n",
    "                    if  len_udf==1 and udf==\"coalesce\":\n",
    "                        query= coalesce_udf(columns,final_df,alias,final_columns,len_udf)\n",
    "                        query_list.append(query)\n",
    "                    elif len_udf==1 and udf==\"mul\":\n",
    "                        query=multiplication_udf(final_columns,alias,final_df)\n",
    "                        query_list.append(query)\n",
    "                    elif len_udf==1 and udf==\"sum\":\n",
    "                        query= sum_initial_udf(columns,final_df,alias)\n",
    "                        query_list.append(query)\n",
    "                        query= group_by_func(final_df,query_dict,grp_cols,alias,udf,final_columns)\n",
    "                        query_list.append(query)\n",
    "                    elif len_udf ==1 and udf==\"year\":\n",
    "                        query=year_month_udf(final_df,alias,udf,final_columns)\n",
    "                        query_list.append(query)\n",
    "                    elif len_udf ==1 and udf==\"month\":\n",
    "                        query=year_month_udf(final_df,alias,udf,final_columns)\n",
    "                        query_list.append(query)\n",
    "                    elif len_udf==1 and udf ==\"literal\":\n",
    "                        query=literals_adjust(final_df,columns,alias)\n",
    "                        query_list.append(query)\n",
    "                    elif len_udf==1 and udf ==\"distinct\":\n",
    "                        query = distinct_unique(final_df,alias,udf,final_columns)\n",
    "                        query_list.append(query)\n",
    "                    elif len_udf==1 and udf==\"count\":\n",
    "                        query= sum_initial_udf(columns,final_df,alias)\n",
    "                        query_list.append(query)\n",
    "                        query= group_by_func(final_df,query_dict,grp_cols,alias,udf,final_columns)\n",
    "                        query_list.append(query)\n",
    "                \n",
    "                        \n",
    "\n",
    "                    elif len_udf>1:\n",
    "                        for udf in reversed(udf_splitter):\n",
    "                            if udf =='mul':\n",
    "                                query=multiplication_udf(final_columns,alias,final_df)\n",
    "                                query_list.append(query)\n",
    "                            elif udf=='sum':\n",
    "                                query= sum_initial_udf(columns,final_df,alias)\n",
    "                                #query_list.append(query)\n",
    "                                query= group_by_func(final_df,query_dict,grp_cols,alias,udf,final_columns)\n",
    "                                query_list.append(query)\n",
    "                            elif udf==\"coalesce\":\n",
    "                                query= coalesce_udf(columns,final_df,alias,final_columns,len_udf)\n",
    "                                query_list.append(query)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    columns =list_elements['base_col']\n",
    "                    alias=list_elements['Alias']\n",
    "                    if alias!='':\n",
    "                        query=final_df+\"[`\"+alias+\"`]\"+\"=\"+final_df+\"[`\"+columns+\"`]\"\n",
    "                        query_list.append(query)\n",
    "        \n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not there\n",
    "\n",
    "def case_alias_getter(query1):\n",
    "    for column_dict in query1:\n",
    "        if type(column_dict)==dict:\n",
    "            if type(column_dict['value'])== dict:\n",
    "                for k,v in column_dict['value'].items():\n",
    "                    if k==\"case\":\n",
    "                        alias = column_dict['name']\n",
    "                        return alias\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be repaired\n",
    "def identifyCaseStatements():\n",
    "        regex = \"CASE(.*?)END\"\n",
    "        allCaseStatements=[]\n",
    "        matches = re.finditer(regex, query, re.IGNORECASE)\n",
    "        for matchNum, match in enumerate(matches, start=1):\n",
    "            allCaseStatements.append(match.group())\n",
    "        return allCaseStatements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not there\n",
    "def case_or_clause(final_df,data):\n",
    "    signs =[\"=\"]\n",
    "    for sign in signs:\n",
    "        if sign in data:\n",
    "            if sign ==\"=\":\n",
    "                data_val=data.split(\"=\")[1]\n",
    "                data_col=data.split(\"=\")[0]\n",
    "                azeta= \"(\"+final_df+\"['\"+data_col+\"']==\"+data_val+\")\"\n",
    "            return azeta\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseStatementDetails(caseStatement):\n",
    "        whenSplits = caseStatement.lower().split(\"when\")[1:]\n",
    "        conditions = [whenSplit.split(\"then\")[0].strip() for whenSplit in whenSplits]\n",
    "        results = [whenSplit.split(\"then\")[1].split(\"else\")[0].strip() for whenSplit in whenSplits]\n",
    "        if \"else\" in caseStatement:\n",
    "            lastResult = caseStatement.split(\"else\")[1].split(\"end\")[0].strip()\n",
    "            results.append(lastResult)\n",
    "        else:\n",
    "            results[-1] = results[-1].split(\"end\")[0].strip()\n",
    "        return conditions, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not there\n",
    "\n",
    "def case_panda_builder(list_of_cols,query_selection,final_df): \n",
    "    panda_case_syntax=[]\n",
    "    for i in list_of_cols:\n",
    "        start = 'else'\n",
    "        end = 'end'\n",
    "        #final_df=\"df\"\n",
    "        else_cond= i[i.find(start)+len(start):i.rfind(end)]\n",
    "        final_else_part=''\n",
    "        if \"/\" in else_cond:\n",
    "            else_splitter=else_cond.split(\"/\")\n",
    "            div_part= final_df+\"['\"+else_splitter[0].strip()+\"']\"+\"/\"+final_df+\"['\"+else_splitter[-1].strip()+\"']\"\n",
    "            alias_assignment= case_alias_getter(query_selection)\n",
    "            #print(alias_assignment)\n",
    "            alias_assignment1 =final_df+\"['\"+alias_assignment+\"']\"\n",
    "            final_else_part=alias_assignment1+\"=\"+div_part\n",
    "        else:\n",
    "            pass\n",
    "        panda_case_syntax.append(final_else_part)\n",
    "        #print(final_else_part)\n",
    "    \n",
    "        conditions,results =caseStatementDetails(i)\n",
    "        then_clause = results[0]\n",
    "        #print(then_clause)\n",
    "        conditions_main=conditions[-1]\n",
    "        case1=\"\"\n",
    "        if \"or\" in conditions_main:\n",
    "            orSplitter = conditions_main.split(\" or \")\n",
    "            when_part=''\n",
    "            for cond in orSplitter:\n",
    "                abc = case_or_clause(final_df,cond)\n",
    "                when_part += abc+\"|\"\n",
    "            #print(\"-----this is when part------\")   \n",
    "            #print(when_part[:-1])\n",
    "            alias_assignment= case_alias_getter(query_selection)\n",
    "            case1=final_df+\".loc[\"+when_part[:-1]+\",\"+\"'\"+alias_assignment+\"']=\"+then_clause\n",
    "            #print((case1))\n",
    "            panda_case_syntax.append(case1)\n",
    "            #cases=cases.append(case1)\n",
    "            \n",
    "            \n",
    "        elif \"and\" in conditions_main:\n",
    "            andSplitter = conditions_main.split(\"and\")\n",
    "        else:\n",
    "            pass\n",
    "        #cases=cases.append(case1)\n",
    "    return panda_case_syntax\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not there\n",
    "def fomrat_udf_substr(select_list, final_df):\n",
    "    data_frame=[]\n",
    "    for column_dict in select_list:\n",
    "        if type(column_dict)==dict:\n",
    "            if type(column_dict['value'])== dict:\n",
    "                for k,v in column_dict['value'].items():\n",
    "                    if k==\"format\":\n",
    "                        alias = column_dict['name']\n",
    "                        values =v\n",
    "                        sliced_col= values[0]\n",
    "                        substr_val= values[-1]\n",
    "                        for k,v in substr_val.items(): # needs to be extended\n",
    "                            data_frame_str = final_df+\"['\"+alias+\"']=\"+final_df+\"['\"+sliced_col+\"'].str[:\"+str(v)+\"]\"\n",
    "                            data_frame.append(data_frame_str)\n",
    "                    \n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs main func starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query =\"\"\" SELECT a.* FROM AW_TGT_PROXY_DBO.IT_SCE8_POP_STD_RM_CAP_HIST AS a INNER JOIN ( select max( data_dt) AS maxdate \n",
    "# FROM AW_TGT_PROXY_DBO.IT_SCE8_POP_STD_RM_CAP_HIST) \n",
    "# AS b ON a.data_dt=b.maxdate WHERE a.set_status='ON'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"SELECT * ,marsha AS marsha2,FORMAT(stay_year, VARCHAR(7)) AS stay_year , marsha2 AS marsha , CASE WHEN co_rev_goal=' ' OR co_rev_goal=' ' OR co_rn_goal=0 OR co_rev_goal=0 THEN 0 ELSE co_rev_goal/co_rn_goal END AS co_rn_goal_adr, CASE WHEN def_otb=' ' OR def_rev=' ' OR def_otb=' ' OR def_rev=0 THEN 0 ELSE def_rev/def_otb END AS def_adr FROM Currents A JOIN Futures B ON A.marsha2 = B.marsha2 AND A.stay_year = B.stay_year\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query =\"\"\"SELECT Marsha, TIMEF, REVPARINDEX as ca, REVPARINDEXLY, RICHG FROM DAYSTAR.master_final  \n",
    "# WHERE TIMEF in( 'L28','WE-28','WD-28','L28_G') AND MARSHA IN ( PropsList)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"\"\" SELECT b.year_cal , COUNT( b.date_dt) AS days_ytd_ly \n",
    "# FROM mrdw_dim_date b WHERE 100*b.year_cal+b.month_cal_id <= 201911-100 AND \n",
    "# 100*b.year_cal+b.month_cal_id >= 201911-112 GROUP BY b.year_cal ORDER BY b.year_cal \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"SELECT distinct prop_code, year( stay_dt) AS year, month( stay_dt) AS month, COUNT( user_capped) AS cap_ct FROM uat1_d GROUP BY prop_code, year, month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"\"\"SELECT distinct prop_code, year( stay_dt) AS year, month( stay_dt) AS month, COUNT( user_capped) AS act_ct FROM uat1_d WHERE act_status='Y' GROUP BY prop_code, year, month\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"SELECT sum(A.alpha) as alpha1, \n",
    "#     coalesce( A.crossover_rms,0) as CO_RN_Goal, \n",
    "#     coalesce( ( A.crossover_rms*B.crossover_gadr),0) as CO_Rev_Goal,\n",
    "#     A.marsha as MARS, \n",
    "#     A.stay_year as stay_year_REN, \n",
    "#     A.CO_RN_Goal, \n",
    "#     A.CO_Rev_Goal, \n",
    "#     A.CO_RN_Goal_ADR, \n",
    "#     A.Def_OTB, \n",
    "#     A.Def_REV, \n",
    "#     A.Def_ADR, \n",
    "#     A.Target, \n",
    "#     A.Avg_Bkd  \n",
    "#     FROM merge_CrossOver1 A \n",
    "#     join merge_CrossOver B \n",
    "#     on A.marsha = B.marsha and A.marsha1 = B.marsha1\n",
    "#     join \n",
    "#     merge_CrossOver2 C on A.marsha = C.marsha and A.marsha1 = C.marsha1 \n",
    "#     Where A.Target=1 \n",
    "#     and A.Target in (1,2,3,4) \n",
    "#     and A.Avg_Bkd=\"ABCD\" \n",
    "# \tgroup by \n",
    "#     crossover_rms, \n",
    "# \tcrossover_gadr,marsha,\n",
    "# \tstay_year,\n",
    "# \tCO_RN_Goal,\n",
    "# \tCO_Rev_Goal,\n",
    "# \tCO_RN_Goal_ADR,\n",
    "# \tDef_OTB,\n",
    "# \tDef_REV,\n",
    "# \tDef_ADR,\n",
    "# \tTarget,\n",
    "# \tAvg_Bkd\n",
    "# \torder by \n",
    "#     A.marsha,A.Avg_Bkd\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*',\n",
       " {'value': 'marsha', 'name': 'marsha2'},\n",
       " {'value': {'format': ['stay_year', {'varchar': 7}]}, 'name': 'stay_year'},\n",
       " {'value': 'marsha2', 'name': 'marsha'},\n",
       " {'value': {'case': [{'when': {'or': [{'eq': ['co_rev_goal',\n",
       "         {'literal': ' '}]},\n",
       "       {'eq': ['co_rev_goal', {'literal': ' '}]},\n",
       "       {'eq': ['co_rn_goal', 0]},\n",
       "       {'eq': ['co_rev_goal', 0]}]},\n",
       "     'then': 0},\n",
       "    {'div': ['co_rev_goal', 'co_rn_goal']}]},\n",
       "  'name': 'co_rn_goal_adr'},\n",
       " {'value': {'case': [{'when': {'or': [{'eq': ['def_otb', {'literal': ' '}]},\n",
       "       {'eq': ['def_rev', {'literal': ' '}]},\n",
       "       {'eq': ['def_otb', {'literal': ' '}]},\n",
       "       {'eq': ['def_rev', 0]}]},\n",
       "     'then': 0},\n",
       "    {'div': ['def_rev', 'def_otb']}]},\n",
       "  'name': 'def_adr'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_simple= query.lower()\n",
    "query_dict = parse(query.lower())\n",
    "select_list = query_dict[\"select\"]\n",
    "select_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"groupby\" in query_dict:\n",
    "    group_section = query_dict['groupby']\n",
    "    group_section\n",
    "else:\n",
    "    group_section=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'base_col': 'marsha', 'udf': '', 'Alias': 'marsha2'},\n",
       " {'base_col': ['stay_year'], 'udf': 'format,varchar', 'Alias': 'stay_year'},\n",
       " {'base_col': 'marsha2', 'udf': '', 'Alias': 'marsha'},\n",
       " {'base_col': [], 'udf': 'format,varchar', 'Alias': 'co_rn_goal_adr'},\n",
       " {'base_col': [], 'udf': 'format,varchar', 'Alias': 'def_adr'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_dict = intermediate_select_dict(select_list)\n",
    "sql_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_agg= group_to_be_applied_on(sql_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = \"df_new\" \n",
    "grp_cols=grouped_columns(group_section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_new[`marsha2`]=df_new[`marsha`]\n",
      "\n",
      "\n",
      "df_new[`marsha`]=df_new[`marsha2`]\n",
      "\n",
      "\n",
      "df_new['stay_year']=df_new['stay_year'].str[:7]\n",
      "df_new['co_rn_goal_adr']=df_new['E WHEN co_rev_goal=' ' OR co_rev_goal=' ' OR co_rn_goal=0 OR co_rev_goal=0 THEN 0 ELSE co_rev_goal']/df_new['co_rn_goal EN']\n",
      "\n",
      "\n",
      "df_new.loc[(df_new['co_rev_goal']==' ')|(df_new['co_rev_goal']==' ')|(df_new['co_rn_goal']==0)|(df_new['co_rev_goal']==0),'co_rn_goal_adr']=0\n",
      "\n",
      "\n",
      "df_new['co_rn_goal_adr']=df_new['E WHEN def_otb=' ' OR def_rev=' ' OR def_otb=' ' OR def_rev=0 THEN 0 ELSE def_rev']/df_new['def_otb EN']\n",
      "\n",
      "\n",
      "df_new.loc[(df_new['def_otb']==' ')|(df_new['def_rev']==' ')|(df_new['def_otb']==' ')|(df_new['def_rev']==0),'co_rn_goal_adr']=0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "panda_queries=pandas_constructor(final_df,grp_cols,sql_dict,group_agg)\n",
    "for queries in panda_queries:\n",
    "    print(queries)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "format_udf=fomrat_udf_substr(select_list, final_df)\n",
    "for queries in format_udf:\n",
    "    print(queries)\n",
    "\n",
    "list_of_cols = identifyCaseStatements()\n",
    "list_of_cols\n",
    "abc=case_panda_builder(list_of_cols,select_list,final_df)\n",
    "for i in abc:\n",
    "    print(i)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'base_col': 'marsha', 'udf': '', 'Alias': 'marsha2'},\n",
       " {'base_col': ['stay_year'], 'udf': 'format,varchar', 'Alias': 'stay_year'},\n",
       " {'base_col': 'marsha2', 'udf': '', 'Alias': 'marsha'},\n",
       " {'base_col': [], 'udf': 'format,varchar', 'Alias': 'co_rn_goal_adr'},\n",
       " {'base_col': [], 'udf': 'format,varchar', 'Alias': 'def_adr'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sql_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-15de52850d8b>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-33-15de52850d8b>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    if udf\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for list_elements in sql_dict:\n",
    "    columns = list_elements['base_col']\n",
    "    if columns!='*':\n",
    "        if columns!=[]:\n",
    "            final_columns =[s for s in columns if type(s)==str]\n",
    "            alias=list_elements['Alias']\n",
    "            udf=list_elements['udf']\n",
    "            udf_splitter = list_elements['udf'].split(',')\n",
    "            len_udf =len(udf_splitter)\n",
    "            \n",
    "            if len_udf>1:\n",
    "                for udf in reversed(udf_splitter):\n",
    "                    if udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_dict in select_list:\n",
    "    if type(column_dict)==dict:\n",
    "        if type(column_dict['value'])== dict:\n",
    "            for k,v in column_dict['value'].items():\n",
    "                if k==\"format\":\n",
    "                    alias = column_dict['name']\n",
    "                    print(alias)\n",
    "                    values =v\n",
    "                    sliced_col= values[0]\n",
    "                    print(sliced_col)\n",
    "                    substr_val= values[-1]\n",
    "                    for k,v in substr_val.items():\n",
    "                        print(v)\n",
    "                        dataFrame = \"d\"+\"  \"+str(v)+\"   \"+alias+\"    \"+sliced_col\n",
    "                        print(alpha)\n",
    "                    \n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fomrat_udf_substr(select_list, final_df):\n",
    "    for column_dict in select_list:\n",
    "        if type(column_dict)==dict:\n",
    "            if type(column_dict['value'])== dict:\n",
    "                for k,v in column_dict['value'].items():\n",
    "                    data_frame=\"\"\n",
    "                    if k==\"format\":\n",
    "                        alias = column_dict['name']\n",
    "                        values =v\n",
    "                        sliced_col= values[0]\n",
    "                        substr_val= values[-1]\n",
    "                        for k,v in substr_val.items():\n",
    "                            data_frame = final_df+\"['\"+alias+\"']=\"+final_df+\"['\"+sliced_col+\"'].str[:\"+str(v)+\"]\"\n",
    "                        return data_frame\n",
    "                    \n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomrat_udf_substr(select_list, final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reader = pd.read_csv(\"C:/Users/arshashank/Desktop/27/SQLQueries_ETL.csv\")\n",
    "reader['alpha']=reader['SQL']\n",
    "for i in reader['alpha']:\n",
    "    print(i)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# reader['sql']\n",
    "# xlf = xlrd.open_workbook('C:/Users/arshashank/Desktop/27/SQLQueries_ETL.csv')\n",
    "# xls = xlf.sheet_by_name('SQLQueries_ETL')\n",
    "# data = xls.col_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
